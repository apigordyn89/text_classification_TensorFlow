{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98af8d77",
   "metadata": {},
   "source": [
    "## Text Classification using TensorFlow\n",
    "\n",
    "#### Objective: Classify question documents into one of 9 categories.\n",
    "\n",
    "#### Author: Juan Gordyn\n",
    "\n",
    "### Importing libraries and checking classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "e347c2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e0b96f",
   "metadata": {},
   "source": [
    "Loading data and checking class proportions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "df0844a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "astronomy           0.244269\n",
       "ai                  0.202527\n",
       "opendata            0.136028\n",
       "sports              0.126375\n",
       "quantumcomputing    0.114400\n",
       "computergraphics    0.074197\n",
       "martialarts         0.045530\n",
       "coffee              0.030647\n",
       "beer                0.026028\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_df = pd.read_csv('questions.csv', header = None, names = ['label', 'text', 'license'])\n",
    "questions_df.label.value_counts()/len(questions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674679c5",
   "metadata": {},
   "source": [
    "### Loading and pre-processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e35a80b",
   "metadata": {},
   "source": [
    "Building class to pre-process the data, split into train and validation so that it is easier afterwards when building the model.\n",
    "\n",
    "For pre-processing, I will keep only words (not numbers, not symbols, etc) and pre-hyphened words (such as state-of-the-art, etc), words containing ' (such as google's) or words unified by an underscore. Stop-words will be kept because I consider that sometimes meaning within the sequence can be altered when removing them. All the words will be lower-cased and lemmatized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "1c690096",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManager:\n",
    "    def __init__(self, maxlen= 50, random_state=6789):\n",
    "        self.numeral_labels = list()\n",
    "        self.maxlen = maxlen\n",
    "        self.numeral_data = list()\n",
    "        self.random_state = random_state\n",
    "        self.random = np.random.RandomState(random_state)\n",
    "    \n",
    "    def preprocessing(self, text_corpus):\n",
    "        # tokenizing by simple words or words combined by _ or -\n",
    "        tokenizer = RegexpTokenizer(r\"[a-zA-Z]+(?:[-'_][a-zA-Z]+)*'?\")\n",
    "        tokenized_text = [tokenizer.tokenize(doc.lower()) for doc in text_corpus]\n",
    "        for i in range(len(tokenized_text)):\n",
    "            tokenized_text[i] = ' '.join([WordNetLemmatizer().lemmatize(token) for token in tokenized_text[i]])\n",
    "        return tokenized_text\n",
    "    \n",
    "    def read_data(self):\n",
    "        # loading data\n",
    "        questions_df = pd.read_csv('questions.csv', header = None, names = ['label', 'text', 'license'])\n",
    "        # pre-processing the data and storing in dataframe\n",
    "        questions_df['text'] = self.preprocessing(questions_df.text)\n",
    "        # converting labels and pre-processed text to lists\n",
    "        self.str_labels = list(questions_df.label)\n",
    "        self.str_questions = list(questions_df.text)\n",
    "         \n",
    "        # turns labels into numbers\n",
    "        le= preprocessing.LabelEncoder()\n",
    "        le.fit(self.str_labels)\n",
    "        # array of labels as numbers\n",
    "        self.numeral_labels = np.array(le.transform(self.str_labels))\n",
    "        # classes to be able to print them whenever we want, as reference\n",
    "        self.str_classes= le.classes_\n",
    "        # number of classes, will be helpful for output layer of NN\n",
    "        self.num_classes= len(self.str_classes)\n",
    "    \n",
    "    def manipulate_data(self):\n",
    "        # tokenizing the pre-processed sequences\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "        tokenizer.fit_on_texts(self.str_questions)\n",
    "        # converting tokens into numerical representation\n",
    "        self.numeral_data = tokenizer.texts_to_sequences(self.str_questions)\n",
    "        # add padding to complete sequence up to max_sequence and truncating when max_sequence is exceeded\n",
    "        self.numeral_data = tf.keras.preprocessing.sequence.pad_sequences(self.numeral_data, padding='post', truncating= 'post', maxlen= self.maxlen)\n",
    "        # building word-index dictionaries\n",
    "        self.word2idx = tokenizer.word_index\n",
    "        self.word2idx = {k:v for k,v in self.word2idx.items()}\n",
    "        self.idx2word = {v:k for k,v in self.word2idx.items()}\n",
    "        # vocab_size = number of unique words in our corpus\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "    \n",
    "    def train_valid_split(self, train_ratio=0.8):\n",
    "        # select indices in random order along it first index to do train-validation split\n",
    "        idxs = np.random.permutation(np.arange(len(self.str_questions)))\n",
    "        # size of training data\n",
    "        train_size = int(train_ratio*len(idxs)) +1\n",
    "        # x train and x val as string questions\n",
    "        self.train_str_questions, self.valid_str_questions = self.str_questions[0:train_size], self.str_questions[train_size:]\n",
    "        # x train and x val as numerical representations\n",
    "        self.train_numeral_data, self.valid_numeral_data = self.numeral_data[0:train_size], self.numeral_data[train_size:]\n",
    "        # y train and y val as numerical labels\n",
    "        self.train_numeral_labels, self.valid_numeral_labels = self.numeral_labels[0:train_size], self.numeral_labels[train_size:]\n",
    "        # generate train and validation sets as tensors to be ingested by Neural Network model\n",
    "        self.tf_train_set = tf.data.Dataset.from_tensor_slices((self.train_numeral_data, self.train_numeral_labels))\n",
    "        self.tf_valid_set = tf.data.Dataset.from_tensor_slices((self.valid_numeral_data, self.valid_numeral_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "81f1b4e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04882936604917151"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking what percentage of questions exceed 100 tokens (to understand if selecting 100 as maxlen is OK)\n",
    "np.sum(questions_df.text.apply(lambda x: len(x))>100)/len(questions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "2a0d3235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only 5%, we can select maxlen=100\n",
    "# implementing the above class\n",
    "dm = DataManager(maxlen=100)\n",
    "dm.read_data()\n",
    "dm.manipulate_data()\n",
    "dm.train_valid_split(train_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "3710b987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized question 1: what is the difference between a qudit system with d and a two-qubit system\n",
      "\n",
      "Numerical representation: [   9    6    1   60   32    2 2902   55   13   85   10    2   70  117\n",
      "   55    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "\n",
      " Label number: 7; Label name: quantumcomputing\n",
      "\n",
      "\n",
      "Tokenized question 2: what doe the sun look like from the heliopause\n",
      "\n",
      "Numerical representation: [   9   18    1   76  250  102   21    1 5096    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "\n",
      " Label number: 1; Label name: astronomy\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# showing how the data is actually encoded\n",
    "for i in range(2):\n",
    "    print('Tokenized question ',i+1,': ', dm.str_questions[i],  '\\n\\nNumerical representation: ', \\\n",
    "          dm.train_numeral_data[i],'\\n\\n Label number: ',dm.train_numeral_labels[i], '; Label name: ', \\\n",
    "          dm.str_labels[i], '\\n','\\n', sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d058339",
   "metadata": {},
   "source": [
    "### Building the model\n",
    "\n",
    "I will build a Recurrent Neural Network model with the following particularities:\n",
    "\n",
    "- run_mode gives 3 possibilities in terms of Word Embeddings: randomly initializing and further training word embeddings (scratch mode), using glove pre-trained word embeddings and freezing their training (init-only), using glove pre-trained word embeddings allowing to further tune their weights (init-fine-tune).\n",
    "- cell_type: being able to use LSTM, GRU or simple RNN.\n",
    "- network_type: uni-directional or bi-directional.\n",
    "- state_sizes: to control the number of hidden layers (final length of the provided list) and their corresponding dimensions (each number of the list).\n",
    "\n",
    "To address overfitting:\n",
    "\n",
    "- dropout_rate: if dropout rate is specified, then dropout will be performed in all the layers. If not specified, then it will not be performed in any.\n",
    "- l2_reg: if this parameter is specified, then l2 regularization will be performed in the last fully connected layer. If not specified, regularization will not be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "dc69ac86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining RNN model\n",
    "class RNN:\n",
    "    def __init__(self, run_mode = 'scratch', cell_type= 'gru', network_type = 'uni-directional', embed_model= 'glove-wiki-gigaword-100', \n",
    "                 embed_size= 128, state_sizes = [64, 64], dropout_rate = None, l2_reg = None, data_manager = None):\n",
    "        self.run_mode = run_mode\n",
    "        self.data_manager = data_manager\n",
    "        self.cell_type = cell_type\n",
    "        self.network_type = network_type\n",
    "        self.state_sizes = state_sizes\n",
    "        self.embed_model = embed_model\n",
    "        self.embed_size = embed_size\n",
    "        # if we are using glove, embed size should be the number indicated in the model's name\n",
    "        if self.run_mode != 'scratch':\n",
    "            self.embed_size = int(self.embed_model.split(\"-\")[-1])\n",
    "        self.data_manager = data_manager\n",
    "        self.vocab_size = dm.vocab_size +1\n",
    "        self.word2idx = dm.word2idx\n",
    "        self.word2vect = None\n",
    "        # initialize embedding matrix as all 0\n",
    "        self.embed_matrix = np.zeros(shape= [self.vocab_size, self.embed_size])\n",
    "        # for regularization\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.l2_reg = l2_reg\n",
    "    \n",
    "    def build_embedding_matrix(self):\n",
    "        self.word2vect = api.load(self.embed_model) # load embedding model\n",
    "        for word, idx in self.word2idx.items():\n",
    "            try:\n",
    "                self.embed_matrix[idx] = self.word2vect.word_vec(word) # assign weight for the corresponding word and index\n",
    "            except KeyError: # word cannot be found\n",
    "                pass\n",
    "    \n",
    "    @staticmethod\n",
    "    # method to build hidden layers with all the combinations of network and cell types\n",
    "    # activation function tanh usually used in RNN\n",
    "    # return_sequences indicates if the concatenation of all hidden values for of all \n",
    "    # hidden cells in addition to output\n",
    "    def get_layer(cell_type= 'gru', state_size= 128, network_type='uni-directional', return_sequences= False, activation = 'tanh'):\n",
    "        if network_type==\"bi-directional\":\n",
    "            if cell_type=='gru':\n",
    "                return tf.keras.layers.Bidirectional(tf.keras.layers.GRU(state_size, return_sequences=return_sequences, activation=activation))\n",
    "            elif cell_type== 'lstm':\n",
    "                return tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(state_size, return_sequences=return_sequences, activation=activation))\n",
    "            else:\n",
    "                return tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(state_size, return_sequences=return_sequences, activation=activation))\n",
    "        else:\n",
    "            if cell_type=='gru':\n",
    "                return tf.keras.layers.GRU(state_size, return_sequences=return_sequences, activation=activation)\n",
    "            elif cell_type== 'lstm':\n",
    "                return tf.keras.layers.LSTM(state_size, return_sequences=return_sequences, activation=activation)\n",
    "            else:\n",
    "                return tf.keras.layers.SimpleRNN(state_size, return_sequences=return_sequences, activation=activation)\n",
    "    def build(self):\n",
    "        x = tf.keras.layers.Input(shape=[None])\n",
    "        if self.run_mode == \"scratch\":\n",
    "            # if scratch, random initialization of embeddings\n",
    "            # mask_zero defines if values padded at the end of the sequence can be ignored for training or not\n",
    "            h = tf.keras.layers.Embedding(self.vocab_size, self.embed_size, mask_zero=True, trainable=True)(x)\n",
    "        else:\n",
    "            if self.run_mode=='init-only':\n",
    "                trainable_param = False\n",
    "            else:\n",
    "                trainable_param = True\n",
    "            self.build_embedding_matrix()\n",
    "            h = tf.keras.layers.Embedding(self.vocab_size, self.embed_size, mask_zero=True, trainable=trainable_param,\n",
    "                                                        weights=[self.embed_matrix])(x)\n",
    "        # we repeat this proccess as many times as number of layers\n",
    "        num_layers = len(self.state_sizes)\n",
    "        for i in range(num_layers):\n",
    "            h =  self.get_layer(self.cell_type, self.state_sizes[i], self.network_type, return_sequences=True)(h)\n",
    "            if self.dropout_rate != None:\n",
    "                # dropout to each hidden layer to control overfitting\n",
    "                h = tf.keras.layers.Dropout(self.dropout_rate)(h)\n",
    "        h = self.get_layer(self.cell_type, self.state_sizes[i], self.network_type, return_sequences=False)(h)\n",
    "        if self.dropout_rate != None:\n",
    "            # dropout to fully connected layer to control overfitting\n",
    "            h = tf.keras.layers.Dropout(self.dropout_rate)(h)\n",
    "        if self.l2_reg != None:\n",
    "            # output layer with softmax with l2 regularization\n",
    "            h = tf.keras.layers.Dense(dm.num_classes, activation='softmax', kernel_regularizer = tf.keras.regularizers.l2(self.l2_reg))(h)\n",
    "        else:\n",
    "            # output layer with softmax without l2 regularization\n",
    "            h = tf.keras.layers.Dense(dm.num_classes, activation='softmax')(h)\n",
    "        self.model = tf.keras.Model(inputs=x, outputs=h)\n",
    "        \n",
    "    def compile_model(self, *args, **kwargs):\n",
    "        self.model.compile(*args, **kwargs)\n",
    "    \n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self.model.fit(*args, **kwargs)\n",
    "    \n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        self.model.evaluate(*args, **kwargs)\n",
    "        \n",
    "    def predict(self, *args, **kwargs):\n",
    "        return self.model.predict(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bdcdce",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "I will train the model using 6 different configurations looping over the different possibilities for cell_type and network_type parameters.\n",
    "\n",
    "I am applying early stopping to avoid overfitting. If the validation loss does not decrease for 3 epochs in a row, the model stops training. I won't apply dropout nor l2_reg at this stage. If I still see signs of overfitting, I will then apply those techniques only to the best-performing model.\n",
    "\n",
    "I have already, on my own, tried the different approaches for the Word Embeddings (scratch, init-only, init-fine-tune) and init-fine-tune was the best performing, so that parameter will be fixed here (to avoid excessive number of different settings)\n",
    "\n",
    "I will be building the model with 2 hidden layers so that it doesn't take excessive training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "01b94c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model1; Training with cell_type=simple-rnn; network_type=uni-directional\n",
      "Epoch 1/10\n",
      "512/512 [==============================] - 38s 73ms/step - loss: 0.8906 - accuracy: 0.6925 - val_loss: 0.7276 - val_accuracy: 0.7616\n",
      "Epoch 2/10\n",
      "512/512 [==============================] - 38s 74ms/step - loss: 0.4795 - accuracy: 0.8436 - val_loss: 0.4839 - val_accuracy: 0.8460\n",
      "Epoch 3/10\n",
      "512/512 [==============================] - 39s 75ms/step - loss: 0.3446 - accuracy: 0.8907 - val_loss: 0.4127 - val_accuracy: 0.8703\n",
      "Epoch 4/10\n",
      "512/512 [==============================] - 37s 73ms/step - loss: 0.2661 - accuracy: 0.9177 - val_loss: 0.3923 - val_accuracy: 0.8811\n",
      "Epoch 5/10\n",
      "512/512 [==============================] - 37s 72ms/step - loss: 0.2098 - accuracy: 0.9343 - val_loss: 0.3839 - val_accuracy: 0.8859\n",
      "Epoch 6/10\n",
      "512/512 [==============================] - 37s 72ms/step - loss: 0.1682 - accuracy: 0.9482 - val_loss: 0.4014 - val_accuracy: 0.8795\n",
      "Epoch 7/10\n",
      "512/512 [==============================] - 37s 72ms/step - loss: 0.1335 - accuracy: 0.9603 - val_loss: 0.3963 - val_accuracy: 0.8896\n",
      "Epoch 8/10\n",
      "512/512 [==============================] - 38s 73ms/step - loss: 0.1067 - accuracy: 0.9676 - val_loss: 0.4325 - val_accuracy: 0.8863\n",
      "Epoch 9/10\n",
      "512/512 [==============================] - 39s 76ms/step - loss: 0.0841 - accuracy: 0.9750 - val_loss: 0.4247 - val_accuracy: 0.8877\n",
      "Epoch 10/10\n",
      "512/512 [==============================] - 38s 74ms/step - loss: 0.0675 - accuracy: 0.9807 - val_loss: 0.4491 - val_accuracy: 0.8857\n",
      "\n",
      "\n",
      "Model2; Training with cell_type=simple-rnn; network_type=bi-directional\n",
      "Epoch 1/10\n",
      "512/512 [==============================] - 61s 119ms/step - loss: 0.7788 - accuracy: 0.7368 - val_loss: 0.6957 - val_accuracy: 0.7683\n",
      "Epoch 2/10\n",
      "512/512 [==============================] - 61s 119ms/step - loss: 0.4132 - accuracy: 0.8670 - val_loss: 0.5072 - val_accuracy: 0.8367\n",
      "Epoch 3/10\n",
      "512/512 [==============================] - 60s 118ms/step - loss: 0.2853 - accuracy: 0.9103 - val_loss: 0.4610 - val_accuracy: 0.8554\n",
      "Epoch 4/10\n",
      "512/512 [==============================] - 61s 119ms/step - loss: 0.2050 - accuracy: 0.9359 - val_loss: 0.4096 - val_accuracy: 0.8758\n",
      "Epoch 5/10\n",
      "512/512 [==============================] - 60s 118ms/step - loss: 0.1469 - accuracy: 0.9549 - val_loss: 0.3929 - val_accuracy: 0.8832\n",
      "Epoch 6/10\n",
      "512/512 [==============================] - 60s 118ms/step - loss: 0.1023 - accuracy: 0.9702 - val_loss: 0.4058 - val_accuracy: 0.8857\n",
      "Epoch 7/10\n",
      "512/512 [==============================] - 60s 118ms/step - loss: 0.0711 - accuracy: 0.9803 - val_loss: 0.4437 - val_accuracy: 0.8820\n",
      "Epoch 8/10\n",
      "512/512 [==============================] - 61s 118ms/step - loss: 0.0481 - accuracy: 0.9859 - val_loss: 0.4464 - val_accuracy: 0.8917\n",
      "Epoch 9/10\n",
      "512/512 [==============================] - 61s 118ms/step - loss: 0.0344 - accuracy: 0.9903 - val_loss: 0.4803 - val_accuracy: 0.8901\n",
      "Epoch 10/10\n",
      "512/512 [==============================] - 60s 118ms/step - loss: 0.0247 - accuracy: 0.9929 - val_loss: 0.5041 - val_accuracy: 0.8887\n",
      "\n",
      "\n",
      "Model3; Training with cell_type=gru; network_type=uni-directional\n",
      "Epoch 1/10\n",
      "512/512 [==============================] - 73s 143ms/step - loss: 0.6526 - accuracy: 0.7787 - val_loss: 0.4459 - val_accuracy: 0.8529\n",
      "Epoch 2/10\n",
      "512/512 [==============================] - 65s 127ms/step - loss: 0.3153 - accuracy: 0.8972 - val_loss: 0.3337 - val_accuracy: 0.8905\n",
      "Epoch 3/10\n",
      "512/512 [==============================] - 63s 123ms/step - loss: 0.2289 - accuracy: 0.9274 - val_loss: 0.2967 - val_accuracy: 0.9054\n",
      "Epoch 4/10\n",
      "512/512 [==============================] - 64s 124ms/step - loss: 0.1768 - accuracy: 0.9448 - val_loss: 0.2862 - val_accuracy: 0.9108\n",
      "Epoch 5/10\n",
      "512/512 [==============================] - 63s 124ms/step - loss: 0.1381 - accuracy: 0.9577 - val_loss: 0.2862 - val_accuracy: 0.9160\n",
      "Epoch 6/10\n",
      "512/512 [==============================] - 63s 122ms/step - loss: 0.1066 - accuracy: 0.9678 - val_loss: 0.3020 - val_accuracy: 0.9178\n",
      "Epoch 7/10\n",
      "512/512 [==============================] - 63s 123ms/step - loss: 0.0808 - accuracy: 0.9767 - val_loss: 0.3204 - val_accuracy: 0.9175\n",
      "Epoch 8/10\n",
      "512/512 [==============================] - 65s 127ms/step - loss: 0.0599 - accuracy: 0.9835 - val_loss: 0.3402 - val_accuracy: 0.9190\n",
      "Epoch 9/10\n",
      "512/512 [==============================] - 66s 129ms/step - loss: 0.0423 - accuracy: 0.9889 - val_loss: 0.3753 - val_accuracy: 0.9169\n",
      "Epoch 10/10\n",
      "512/512 [==============================] - 65s 127ms/step - loss: 0.0294 - accuracy: 0.9926 - val_loss: 0.4159 - val_accuracy: 0.9153\n",
      "\n",
      "\n",
      "Model4; Training with cell_type=gru; network_type=bi-directional\n",
      "Epoch 1/10\n",
      "512/512 [==============================] - 120s 234ms/step - loss: 0.5665 - accuracy: 0.8076 - val_loss: 0.4269 - val_accuracy: 0.8536\n",
      "Epoch 2/10\n",
      "512/512 [==============================] - 112s 220ms/step - loss: 0.2929 - accuracy: 0.9033 - val_loss: 0.3160 - val_accuracy: 0.8962\n",
      "Epoch 3/10\n",
      "512/512 [==============================] - 112s 219ms/step - loss: 0.2113 - accuracy: 0.9321 - val_loss: 0.2840 - val_accuracy: 0.9121\n",
      "Epoch 4/10\n",
      "512/512 [==============================] - 111s 218ms/step - loss: 0.1587 - accuracy: 0.9509 - val_loss: 0.2777 - val_accuracy: 0.9160\n",
      "Epoch 5/10\n",
      "512/512 [==============================] - 111s 216ms/step - loss: 0.1182 - accuracy: 0.9642 - val_loss: 0.2917 - val_accuracy: 0.9181\n",
      "Epoch 6/10\n",
      "512/512 [==============================] - 111s 218ms/step - loss: 0.0847 - accuracy: 0.9747 - val_loss: 0.3250 - val_accuracy: 0.9145\n",
      "Epoch 7/10\n",
      "512/512 [==============================] - 110s 216ms/step - loss: 0.0590 - accuracy: 0.9825 - val_loss: 0.3618 - val_accuracy: 0.9142\n",
      "Epoch 8/10\n",
      "512/512 [==============================] - 111s 218ms/step - loss: 0.0395 - accuracy: 0.9891 - val_loss: 0.3844 - val_accuracy: 0.9168\n",
      "\n",
      "\n",
      "Model5; Training with cell_type=lstm; network_type=uni-directional\n",
      "Epoch 1/10\n",
      "512/512 [==============================] - 81s 158ms/step - loss: 0.8335 - accuracy: 0.7187 - val_loss: 0.5253 - val_accuracy: 0.8276\n",
      "Epoch 2/10\n",
      "512/512 [==============================] - 77s 150ms/step - loss: 0.3785 - accuracy: 0.8755 - val_loss: 0.3762 - val_accuracy: 0.8785\n",
      "Epoch 3/10\n",
      "512/512 [==============================] - 77s 150ms/step - loss: 0.2607 - accuracy: 0.9157 - val_loss: 0.3072 - val_accuracy: 0.9014\n",
      "Epoch 4/10\n",
      "512/512 [==============================] - 76s 148ms/step - loss: 0.1978 - accuracy: 0.9374 - val_loss: 0.2884 - val_accuracy: 0.9118\n",
      "Epoch 5/10\n",
      "512/512 [==============================] - 76s 149ms/step - loss: 0.1549 - accuracy: 0.9514 - val_loss: 0.2847 - val_accuracy: 0.9165\n",
      "Epoch 6/10\n",
      "512/512 [==============================] - 77s 151ms/step - loss: 0.1217 - accuracy: 0.9633 - val_loss: 0.2907 - val_accuracy: 0.9204\n",
      "Epoch 7/10\n",
      "512/512 [==============================] - 80s 156ms/step - loss: 0.0946 - accuracy: 0.9718 - val_loss: 0.3103 - val_accuracy: 0.9179\n",
      "Epoch 8/10\n",
      "512/512 [==============================] - 76s 149ms/step - loss: 0.0711 - accuracy: 0.9798 - val_loss: 0.3337 - val_accuracy: 0.9151\n",
      "Epoch 9/10\n",
      "512/512 [==============================] - 79s 155ms/step - loss: 0.0522 - accuracy: 0.9860 - val_loss: 0.3615 - val_accuracy: 0.9154\n",
      "\n",
      "\n",
      "Model6; Training with cell_type=lstm; network_type=bi-directional\n",
      "Epoch 1/10\n",
      "512/512 [==============================] - 129s 252ms/step - loss: 0.6576 - accuracy: 0.7804 - val_loss: 0.4726 - val_accuracy: 0.8463\n",
      "Epoch 2/10\n",
      "512/512 [==============================] - 132s 258ms/step - loss: 0.3300 - accuracy: 0.8932 - val_loss: 0.3480 - val_accuracy: 0.8895\n",
      "Epoch 3/10\n",
      "512/512 [==============================] - 125s 244ms/step - loss: 0.2337 - accuracy: 0.9263 - val_loss: 0.3172 - val_accuracy: 0.9033\n",
      "Epoch 4/10\n",
      "512/512 [==============================] - 125s 245ms/step - loss: 0.1760 - accuracy: 0.9455 - val_loss: 0.3052 - val_accuracy: 0.9102\n",
      "Epoch 5/10\n",
      "512/512 [==============================] - 124s 242ms/step - loss: 0.1358 - accuracy: 0.9587 - val_loss: 0.3155 - val_accuracy: 0.9135\n",
      "Epoch 6/10\n",
      "512/512 [==============================] - 126s 246ms/step - loss: 0.1036 - accuracy: 0.9687 - val_loss: 0.3279 - val_accuracy: 0.9147\n",
      "Epoch 7/10\n",
      "512/512 [==============================] - 127s 247ms/step - loss: 0.0796 - accuracy: 0.9765 - val_loss: 0.3292 - val_accuracy: 0.9196\n",
      "Epoch 8/10\n",
      "512/512 [==============================] - 126s 246ms/step - loss: 0.0650 - accuracy: 0.9812 - val_loss: 0.3664 - val_accuracy: 0.9136\n",
      "Epoch 9/10\n",
      "512/512 [==============================] - 124s 243ms/step - loss: 0.0477 - accuracy: 0.9864 - val_loss: 0.3909 - val_accuracy: 0.9131\n",
      "Epoch 10/10\n",
      "512/512 [==============================] - 125s 245ms/step - loss: 0.0381 - accuracy: 0.9888 - val_loss: 0.3968 - val_accuracy: 0.9162\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initializing model index\n",
    "i = 0\n",
    "# initializing Data Frame to store performances\n",
    "results_df = pd.DataFrame()\n",
    "# looping all the possible values for the parameters\n",
    "for cell_type in ['simple-rnn', 'gru', 'lstm']:\n",
    "    for network_type in ['uni-directional', 'bi-directional']:\n",
    "        print('Model', i+1, '; Training with cell_type=', cell_type, '; network_type=',network_type, sep='')\n",
    "        rnn = RNN(run_mode= 'init-fine-tune', data_manager = dm, cell_type=cell_type, network_type=network_type)\n",
    "        rnn.build()\n",
    "        opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "        callback = EarlyStopping(patience=3, monitor='val_accuracy', mode='max')\n",
    "        rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        history = rnn.fit(dm.tf_train_set.batch(64), epochs=10, validation_data = dm.tf_valid_set.batch(64), callbacks = [callback])\n",
    "        # keeping the number of epochs that yields the highest val accuracy for each model\n",
    "        valid_accuracies = history.history['val_accuracy']\n",
    "        max_accuracy = np.max(valid_accuracies)\n",
    "        max_epochs = np.argmax(valid_accuracies)\n",
    "        # storing all results in DataFrame\n",
    "        results_df.loc[i, 'cell_type'] = cell_type\n",
    "        results_df.loc[i, 'netowrk_type'] = network_type\n",
    "        results_df.loc[i, 'optimal_epoch'] = max_epochs + 1\n",
    "        results_df.loc[i, 'optimal_accuracy'] = max_accuracy\n",
    "        i+=1\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c217717",
   "metadata": {},
   "source": [
    "We can see that there are signs of overfitting in all the models: our training accuracy is always pretty close to 1 while our validation accuracy is somewhere between 0.89 and 0.92. I will then select the best performing model (the one with the optimal validation accuracy) and apply dropout and regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37421591",
   "metadata": {},
   "source": [
    "### The results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1f5a2ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_type</th>\n",
       "      <th>netowrk_type</th>\n",
       "      <th>optimal_epoch</th>\n",
       "      <th>optimal_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>simple-rnn</td>\n",
       "      <td>uni-directional</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.889649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>simple-rnn</td>\n",
       "      <td>bi-directional</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.891727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gru</td>\n",
       "      <td>uni-directional</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.918978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gru</td>\n",
       "      <td>bi-directional</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.918123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lstm</td>\n",
       "      <td>uni-directional</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.920445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lstm</td>\n",
       "      <td>bi-directional</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.919589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cell_type     netowrk_type  optimal_epoch  optimal_accuracy\n",
       "0  simple-rnn  uni-directional            7.0          0.889649\n",
       "1  simple-rnn   bi-directional            8.0          0.891727\n",
       "2         gru  uni-directional            8.0          0.918978\n",
       "3         gru   bi-directional            5.0          0.918123\n",
       "4        lstm  uni-directional            6.0          0.920445\n",
       "5        lstm   bi-directional            7.0          0.919589"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4111c816",
   "metadata": {},
   "source": [
    "We can observe that our best-performing model is lstm uni-directional. So let's re-train it, addressing the overfitting problem this time with dropout and regularization, save it as the optimal model and take a look at the predictions to see if it is working well in all the classes.\n",
    "\n",
    "I will first define a scheduler for the learning rate, to decrease as the training progresses (to speed up the performance at the beginning and avoid overshooting once the training is advanced)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "bc8a4604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining scheduler for learning rate\n",
    "def scheduler(epoch, learning_rate):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 8:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "a0ad181f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "512/512 [==============================] - 83s 162ms/step - loss: 0.6243 - accuracy: 0.8054 - val_loss: 0.3264 - val_accuracy: 0.9077\n",
      "Epoch 2/10\n",
      "512/512 [==============================] - 79s 155ms/step - loss: 0.2833 - accuracy: 0.9255 - val_loss: 0.3040 - val_accuracy: 0.9186\n",
      "Epoch 3/10\n",
      "512/512 [==============================] - 80s 155ms/step - loss: 0.2072 - accuracy: 0.9479 - val_loss: 0.3022 - val_accuracy: 0.9234\n",
      "Epoch 4/10\n",
      "512/512 [==============================] - 80s 156ms/step - loss: 0.1716 - accuracy: 0.9579 - val_loss: 0.3215 - val_accuracy: 0.9248\n",
      "Epoch 5/10\n",
      "512/512 [==============================] - 79s 154ms/step - loss: 0.1458 - accuracy: 0.9654 - val_loss: 0.3663 - val_accuracy: 0.9258\n",
      "Epoch 6/10\n",
      "512/512 [==============================] - 80s 155ms/step - loss: 0.1040 - accuracy: 0.9751 - val_loss: 0.3604 - val_accuracy: 0.9294\n",
      "Epoch 7/10\n",
      "512/512 [==============================] - 80s 156ms/step - loss: 0.0909 - accuracy: 0.9782 - val_loss: 0.3708 - val_accuracy: 0.9296\n",
      "Epoch 8/10\n",
      "512/512 [==============================] - 80s 157ms/step - loss: 0.0815 - accuracy: 0.9809 - val_loss: 0.3791 - val_accuracy: 0.9308\n",
      "Epoch 9/10\n",
      "512/512 [==============================] - 81s 158ms/step - loss: 0.0767 - accuracy: 0.9821 - val_loss: 0.3812 - val_accuracy: 0.9306\n",
      "Epoch 10/10\n",
      "512/512 [==============================] - 81s 158ms/step - loss: 0.0776 - accuracy: 0.9830 - val_loss: 0.3828 - val_accuracy: 0.9301\n"
     ]
    }
   ],
   "source": [
    "# re-running model on optimal parameters, adding lr scheduler, dropout_rate and l2_reg\n",
    "optimal_rnn = RNN(run_mode= 'init-fine-tune', data_manager = dm, cell_type='lstm', network_type='uni-directional',\\\n",
    "                 dropout_rate = 0.5, l2_reg = 0.0001)\n",
    "optimal_rnn.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "# learning rate scheduler\n",
    "scheduler_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "# early stopping\n",
    "early_stopping = EarlyStopping(patience=3, monitor='val_accuracy', mode='max')\n",
    "optimal_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history = optimal_rnn.fit(dm.tf_train_set.batch(64), epochs=10, validation_data = dm.tf_valid_set.batch(64),\\\n",
    "                          callbacks = [scheduler_lr,  early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf90d07",
   "metadata": {},
   "source": [
    "We can see in our results that we have obtained a higher validation accuracy (0.93 now vs 0.92 in the previous optimal model without l2_reg, dropout nor lr_scheduler) and we have managed to reduce overfitting: the difference between training and validation accuracy is now smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "e296fd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using predict method to be able to actually retrieve the predictions and not only the performance\n",
    "predictions = optimal_rnn.predict(dm.valid_numeral_data)\n",
    "predictions_list = []\n",
    "for i in range(len(predictions)):\n",
    "    predictions_list.append(np.argmax(predictions[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "4ade0256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[6449,  138],\n",
       "        [ 142, 1454]],\n",
       "\n",
       "       [[6106,   75],\n",
       "        [  85, 1917]],\n",
       "\n",
       "       [[7951,   11],\n",
       "        [  23,  198]],\n",
       "\n",
       "       [[7929,   13],\n",
       "        [  17,  224]],\n",
       "\n",
       "       [[7451,   92],\n",
       "        [  73,  567]],\n",
       "\n",
       "       [[7739,   36],\n",
       "        [  38,  370]],\n",
       "\n",
       "       [[7003,   82],\n",
       "        [  89, 1009]],\n",
       "\n",
       "       [[7163,   63],\n",
       "        [  50,  907]],\n",
       "\n",
       "       [[7101,   62],\n",
       "        [  55,  965]]])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building confusion matrices for each class\n",
    "multilabel_confusion_matrix(dm.valid_numeral_labels, predictions_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c26512",
   "metadata": {},
   "source": [
    "By taking a look at the Confusion Matrices we can get an idea that the model is performing quite good in all the classes. Still we can calculate each class fscore to confirm this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "ac74c759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.91217064, 0.95993991, 0.92093023, 0.93723849, 0.87297921,\n",
       "       0.90909091, 0.92188214, 0.94135963, 0.94284319])"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(dm.valid_numeral_labels, predictions_list, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "0bf0dff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ai', 'astronomy', 'beer', 'coffee', 'computergraphics',\n",
       "       'martialarts', 'opendata', 'quantumcomputing', 'sports'],\n",
       "      dtype='<U16')"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.str_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1306497c",
   "metadata": {},
   "source": [
    "We see that the model is actually doing pretty good in all the classes. computergraphics is not performing as well as the others, so we could dig deeper and see why this is happening (maybe we could change data pre-processing, this time including numbers as well and see if this boosts the performance in this class, because numbers should be meaningful in a domain such as computergraphics, etc...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
